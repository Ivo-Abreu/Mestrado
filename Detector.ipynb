{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4a578b",
   "metadata": {},
   "source": [
    "# Detecção em duas fases\n",
    "Modelos do scikit-learn, parâmetros para criação de cenários, pre-processamento dos dados, aprendizado dos modelos e predições.  \n",
    "Gera arquivo para análise e gráficos para uso em outro caderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e95755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from math import ceil\n",
    "from time import time\n",
    "from timer import timer\n",
    "# from tqdm.notebook import tnrange,tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score,roc_auc_score,confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # cuidado com versões > 1.2.2: bug c_contiguous\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "# Parâmetros de execução\n",
    "table_name = \"table4.csv\" # Nome do arquivo a ser gerado\n",
    "path_to_datasets = 'datasets' # Caminho das derivações\n",
    "random_state = 2023 # Parâmetro de aleatorização\n",
    "n_jobs = 14 # Cores a usar quando possível\n",
    "n_iter = 50 # Quantidade de iterações\n",
    "\n",
    "# Parâmetros para construção dos cenários\n",
    "test_sizes = [0.5,0.35,0.1] # floats, 0.0 < valor <= 1.0\n",
    "hits = [0.05,0.1,0.25,0.5] # floats, 0.0 < valor < 1.0\n",
    "samples_sizes = [5000] # valor automático adicionado posteriormente, int ou float entre 0 e 1\n",
    "\n",
    "# Valores para amostragem pela estimativa da média populacional (https://pessoal.dainf.ct.utfpr.edu.br/maurofonseca/doku.php?id=cursos:sadrc:exdimesionamento)\n",
    "E = 0.01 # Erro T-Student\n",
    "GL = 1.645 # G.L. da tabela (no caso infinita amostras, 90% bicaudal)\n",
    "\n",
    "# Parâmetros de modelos\n",
    "lr_d = 18 # Limiar de dimensionalidade para mudança de solver do Logistic Regression\n",
    "knns = [2,5,10] # Lista para \"k\" do KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec8101",
   "metadata": {},
   "source": [
    "Função para executar o aprendizado e predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154d14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X,Y,models,n_iter:int=1,n_samples:int=1,random_state:int=0,test_size:float=0.5,hit:float=0.1):\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    resultados = []\n",
    "    for name,clf in models.items():\n",
    "        XY = pd.concat([X,Y],axis=1)\n",
    "        for i in range(n_iter):\n",
    "            if 'train' in XY.columns:\n",
    "                X_o = XY[XY.Attack == 1].sample(n=int(n_samples*hit),random_state=random_state+i)\n",
    "                X_o_train = X_o.sample(frac=(1-test_size))\n",
    "                X_o_test = X_o[~X_o.isin(X_o_train).all(1)]\n",
    "                X_i_train = XY[XY.train == 1]\n",
    "                X_i_train = X_i_train.sample(n=int(n_samples*(1-test_size))-X_o_train.shape[0],random_state=random_state+i)\n",
    "                X_i_test = XY[XY.train == 0]\n",
    "                X_i_test = X_i_test[X_i_test.Attack == 0].sample(n=int(n_samples*test_size)-X_o_test.shape[0],random_state=random_state+i)\n",
    "                X_train = pd.concat([X_o_train,X_i_train],axis=0)\n",
    "                X_test = pd.concat([X_o_test,X_i_test],axis=0)\n",
    "                Y_train = X_train['Attack']\n",
    "                Y_test = X_test['Attack']\n",
    "                X_train.drop(columns=['train','Attack'],inplace=True)\n",
    "                X_test.drop(columns=['train','Attack'],inplace=True)\n",
    "            else:\n",
    "                X_o = XY[XY.Attack == 1].sample(n=int(n_samples*hit),random_state=random_state+i)\n",
    "                X_i = XY[XY.Attack == 0].sample(n=n_samples-X_o.shape[0],random_state=random_state+i)\n",
    "                X_ = pd.concat([X_i,X_o],axis=0).reset_index(drop=True)\n",
    "                Y_ = X_['Attack']\n",
    "                X_ = X_.drop(columns=['Attack'])\n",
    "                X_train,X_test,Y_train,Y_test = train_test_split(X_,Y_,random_state=random_state,stratify=Y_,test_size=test_size)\n",
    "            \n",
    "            if type(clf) == type(OneClassSVM()) or type(clf) == type(LocalOutlierFactor()) and clf.novelty:\n",
    "                X_train = X_train[Y_train == 0]\n",
    "                start = time()\n",
    "                clf.fit(X_train)\n",
    "                lap = time()\n",
    "                Y_pred = clf.predict(X_test)\n",
    "                stop = time()\n",
    "                Y_pred[Y_pred>0] = 0\n",
    "                Y_pred[Y_pred<0] = 1\n",
    "            elif type(clf) == type(IsolationForest()) or type(clf) == type(LocalOutlierFactor()):\n",
    "                start = time()\n",
    "                Y_pred = clf.fit_predict(X_test)\n",
    "                stop = time()\n",
    "                lap = start\n",
    "                Y_pred[Y_pred>0] = 0\n",
    "                Y_pred[Y_pred<0] = 1\n",
    "            else:\n",
    "                start = time()\n",
    "                clf.fit(X_train,Y_train)\n",
    "                lap = time()\n",
    "                Y_pred = clf.predict(X_test)\n",
    "                stop = time()\n",
    "            accuracy = accuracy_score(y_true=Y_test,y_pred=Y_pred)\n",
    "            recall = recall_score(y_true=Y_test,y_pred=Y_pred)\n",
    "            f1 = f1_score(y_true=Y_test,y_pred=Y_pred)\n",
    "            roc_auc = roc_auc_score(y_true=Y_test,y_score=Y_pred)\n",
    "            train_time = lap - start\n",
    "            test_time = stop - lap\n",
    "            cm = confusion_matrix(y_true=Y_test,y_pred=Y_pred)\n",
    "            fp = (cm.sum(axis=0) - np.diag(cm))[1]\n",
    "            resultado = [name,i,train_time,test_time,fp,accuracy,recall,f1,\n",
    "                         roc_auc,Y_test,Y_pred]\n",
    "            resultados.append(resultado)\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8e5ea",
   "metadata": {},
   "source": [
    "Separação dos modelos segundo tarefa e pacote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4d3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised scikit-learn\n",
    "models_sup_sk = {\"Perceptron\":Perceptron(n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"LogisticRegression\":LogisticRegression(max_iter=400,n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"SVC-Linear\":SVC(kernel='linear',random_state=random_state),\n",
    "                 \"SVC-RBF\":SVC(kernel='rbf',random_state=random_state),\n",
    "                 \"SGDClassifier\":SGDClassifier(n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=random_state),\n",
    "                 \"RandomForestClassifier\":RandomForestClassifier(n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"AdaBoostClassifier\":AdaBoostClassifier(random_state=random_state),\n",
    "                 \"MLPClassifier\":MLPClassifier(random_state=random_state),\n",
    "                 \"GaussianProcessClassifier\":GaussianProcessClassifier(n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"GaussianNB\":GaussianNB(),\n",
    "                 \"QuadraticDiscriminantAnalysis\":QuadraticDiscriminantAnalysis(),\n",
    "                 \"ExtraTrees\":ExtraTreesClassifier(n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"Bagging-DTree\":BaggingClassifier(DecisionTreeClassifier(),n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"Bagging-RF\":BaggingClassifier(RandomForestClassifier(),n_jobs=n_jobs,random_state=random_state),\n",
    "                 \"Bagging-ETree\":BaggingClassifier(ExtraTreesClassifier(),n_jobs=n_jobs,random_state=random_state)}\n",
    "\n",
    "# Unsupervised scikit-learn                 \n",
    "models_unsup_sk = {\"OneClassSVM-Linear\":OneClassSVM(kernel='linear'),\n",
    "                   \"OneClassSVM-RBF\":OneClassSVM(kernel='rbf'),\n",
    "                   \"LOF-Novelty\":LocalOutlierFactor(novelty=True,n_jobs=n_jobs),\n",
    "                   \"LOF\":LocalOutlierFactor(n_jobs=n_jobs),\n",
    "                   \"IsolationForest\":IsolationForest(n_jobs=n_jobs,random_state=random_state),\n",
    "                   \"OneClassSVM-Linear-C\":OneClassSVM(kernel='linear'),\n",
    "                   \"OneClassSVM-RBF-C\":OneClassSVM(kernel='rbf'),\n",
    "                   \"LOF-Novelty-C\":LocalOutlierFactor(novelty=True,n_jobs=n_jobs),\n",
    "                   \"LOF-C\":LocalOutlierFactor(n_jobs=n_jobs),\n",
    "                   \"IsolationForest-C\":IsolationForest(n_jobs=n_jobs,random_state=random_state)}\n",
    "\n",
    "for knn in knns:\n",
    "        models_sup_sk[\"KNN-\"+str(knn)] = KNeighborsClassifier(n_neighbors=knn,n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a3da6",
   "metadata": {},
   "source": [
    "Obtenção de lista dos nomes dos datasets para carga em memória conforme demanda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86dee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga da lista de datasets do diretório\n",
    "dataset_list = [os.path.splitext(_)[0] for _ in os.listdir(path_to_datasets) \n",
    "                if os.path.splitext(_)[1] == '.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8338329",
   "metadata": {},
   "source": [
    "Criação de variável para armazenar quantidade de amostras segundo estimativa populacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d01f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_sample = {}\n",
    "for dataset in dataset_list:\n",
    "    df = pd.read_csv(path_to_datasets+\"/\"+dataset+\".csv\")\n",
    "    if 'train' in df.columns:\n",
    "        df.drop(columns=['train'],inplace=True)\n",
    "    df.drop(columns=['Attack'],inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.DataFrame(scaler.fit_transform(df))\n",
    "    dvp = max(df.std())\n",
    "    n = (GL * dvp / E)**2\n",
    "    auto_sample[dataset] = ceil(n)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321de4c0",
   "metadata": {},
   "source": [
    "## Criação da tabela de resultados da detecção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3aff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [114:25:25, 1144.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 4d 18h 25m 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "while 0 in samples_sizes:\n",
    "    samples_sizes.pop(0)\n",
    "samples_sizes.append(0) # utilizado para valor padrão de cada derivação\n",
    "samples_sizes.append(-1) # introduzido para dobrar o auto-smaples\n",
    "experiment_params = product(dataset_list,samples_sizes,test_sizes,hits)\n",
    "table = {\"HAI\":[],\n",
    "         \"Files\":[],\n",
    "         \"Selector\":[],\n",
    "         \"N_Samples\":[],\n",
    "         \"Test_Size\":[],\n",
    "         \"Contamination\":[],\n",
    "         \"Classifier_1s\":[],\n",
    "         \"Classifier_2s\":[],\n",
    "         \"Accuracy\":[],\n",
    "         \"Recall\":[],\n",
    "         \"F1\":[],\n",
    "         \"AUC\":[],\n",
    "         \"FP\":[],\n",
    "         \"FN\":[],\n",
    "         \"FPR\":[],\n",
    "         \"FNR\":[],\n",
    "         \"Train_Time\":[],\n",
    "         \"Test_Time\":[],\n",
    "         \"Iteration\":[]}\n",
    "done = []\n",
    "has_tag = False\n",
    "tic = time()\n",
    "\n",
    "for dataset,sample_size,test_size,hit_ in tqdm(experiment_params):\n",
    "    # Carga do dataset\n",
    "    df = pd.read_csv(path_to_datasets+\"/\"+dataset+\".csv\")\n",
    "    Y = df['Attack']\n",
    "    if 'train' in df:\n",
    "        tag = df['train']\n",
    "        has_tag = True\n",
    "        df.drop(columns=['train'],inplace=True)\n",
    "    X = df.drop(columns=['Attack'])\n",
    "    del df\n",
    "\n",
    "    # Normalização\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X))\n",
    "    if has_tag:\n",
    "        X['train'] = tag\n",
    "        has_tag = False\n",
    "\n",
    "    # Tamanho da amostra\n",
    "    if sample_size == 0:\n",
    "        n_samples = auto_sample[dataset]\n",
    "    elif sample_size == -1:\n",
    "        n_samples = 2 * auto_sample[dataset]\n",
    "    elif type(sample_size) == float:\n",
    "        n_samples =  int(len(Y) * sample_size)\n",
    "    elif type(sample_size) == int:\n",
    "        n_samples = sample_size\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    if n_samples < 2:\n",
    "        n_samples = 2\n",
    "    if n_samples > len(Y):\n",
    "        n_samples = len(Y)\n",
    "\n",
    "    # Teste de restrição de hit/test_size\n",
    "    if int(hit_*n_samples) > Y.sum():\n",
    "        hit = Y.sum()/n_samples\n",
    "    else:\n",
    "        hit = hit_\n",
    "    \n",
    "    todo = (dataset,n_samples,test_size,hit)\n",
    "    if todo in done:\n",
    "        continue\n",
    "    \n",
    "    if X.shape[1] > lr_d:\n",
    "        models_sup_sk[\"LogisticRegression\"].set_params(**{\"solver\":\"sag\"})\n",
    "    else:\n",
    "        models_sup_sk[\"LogisticRegression\"].set_params(**{\"solver\":\"lbfgs\"})\n",
    "    resultado_sup = run(X,Y,models_sup_sk,\n",
    "                    n_iter=n_iter,\n",
    "                    n_samples=n_samples,\n",
    "                    random_state=random_state,\n",
    "                    test_size=test_size,\n",
    "                    hit=hit)\n",
    "    \n",
    "    for model in models_unsup_sk:\n",
    "        if \"-C\" in model:\n",
    "            if type(models_unsup_sk[model]) == type(OneClassSVM()):\n",
    "                models_unsup_sk[model].set_params(**{'nu':hit})\n",
    "            elif type(models_unsup_sk[model]) == type(LocalOutlierFactor()):\n",
    "                models_unsup_sk[model].set_params(**{'contamination':hit})\n",
    "            elif type(models_unsup_sk[model]) == type(IsolationForest()):\n",
    "                models_unsup_sk[model].set_params(**{'contamination':hit})\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    resultado_unsup = run(X,Y,models_unsup_sk,\n",
    "                    n_iter=n_iter,\n",
    "                    n_samples=n_samples,\n",
    "                    random_state=random_state,\n",
    "                    test_size=test_size,\n",
    "                    hit=hit)\n",
    "    \n",
    "    # Composição\n",
    "    data_1s = pd.DataFrame(resultado_sup,columns=['Classifier',\n",
    "                                                  'iteration',\n",
    "                                                  'train_time',\n",
    "                                                  'test_time',\n",
    "                                                  'FP',\n",
    "                                                  'Accuracy','Recall','F1',\n",
    "                                                  'ROC AUC',\n",
    "                                                  'Y_true','Y_pred'])\n",
    "    data_2s = pd.DataFrame(resultado_unsup,columns=['Classifier',\n",
    "                                                    'iteration',\n",
    "                                                    'train_time',\n",
    "                                                    'test_time',\n",
    "                                                    'FP',\n",
    "                                                    'Accuracy','Recall','F1',\n",
    "                                                    'ROC AUC',\n",
    "                                                    'Y_true','Y_pred'])\n",
    "    n_test = data_1s.loc[0,'Y_true'].size\n",
    "    n_hit = data_1s.loc[0,'Y_true'].sum()\n",
    "    chain = []\n",
    "    for clf_1s in data_1s['Classifier'].unique():\n",
    "        for iter in range(n_iter):\n",
    "            dataux = data_1s.query(\"Classifier == '\"+clf_1s+\"' and iteration == \"+str(iter))\n",
    "            Y_pred_1 = dataux['Y_pred'].to_list()[0]\n",
    "            Y_test = dataux['Y_true'].to_list()[0]\n",
    "            ttr1 = dataux['train_time'].values[0]\n",
    "            tte1 = dataux['test_time'].values[0]\n",
    "            fp0 = dataux['FP'].values[0]\n",
    "            cm = confusion_matrix(y_true=Y_test,y_pred=Y_pred_1)\n",
    "            fn0 = (cm.sum(axis=1) - np.diag(cm))[1]\n",
    "            score_acc = dataux['Accuracy'].values[0]\n",
    "            score_recall = dataux['Recall'].values[0]\n",
    "            score_f1 = dataux['F1'].values[0]\n",
    "            score_rocauc = dataux['ROC AUC'].values[0]\n",
    "            link = [clf_1s,iter,\"1step_only\",\n",
    "                    ttr1,tte1,fp0,fn0,\n",
    "                    score_acc,score_recall,score_f1,\n",
    "                    score_rocauc,\n",
    "                    Y_test,Y_pred_1]\n",
    "            chain.append(link)\n",
    "            for od in data_2s['Classifier'].unique():\n",
    "                dataux = data_2s.query(\"Classifier == '\"+od+\"' and iteration == \"+str(iter))\n",
    "                Y_pred_2 = dataux['Y_pred'].to_list()[0]\n",
    "                Y_test = dataux['Y_true'].to_list()[0]\n",
    "                Y_pred_3 = Y_pred_1 + Y_pred_2\n",
    "                Y_pred_3[Y_pred_3 == 2] = 1\n",
    "                ttr2 = dataux['train_time'].values[0]\n",
    "                tte2 = dataux['test_time'].values[0]\n",
    "                cm = confusion_matrix(y_true=Y_test,y_pred=Y_pred_3)\n",
    "                fp = (cm.sum(axis=0) - np.diag(cm))[1]\n",
    "                fn = (cm.sum(axis=1) - np.diag(cm))[1]\n",
    "                score_acc = accuracy_score(Y_test,Y_pred_3)\n",
    "                score_recall = recall_score(Y_test,Y_pred_3,zero_division=0)\n",
    "                score_f1 = f1_score(Y_test,Y_pred_3,zero_division=0)\n",
    "                score_rocauc = roc_auc_score(Y_test,Y_pred_3)\n",
    "                link = [clf_1s,iter,od,\n",
    "                        ttr1+ttr2,tte1+tte2,fp,fn,\n",
    "                        score_acc,score_recall,score_f1,\n",
    "                        score_rocauc,\n",
    "                        Y_test,Y_pred_3]\n",
    "                chain.append(link)\n",
    "\n",
    "    name = dataset.split('_')\n",
    "    for link in chain:\n",
    "        table[\"HAI\"].append(name[0])\n",
    "\n",
    "        if 'tr' in name[1]:\n",
    "            table['Files'].append('_'.join([name[1],name[2]]))\n",
    "        else:\n",
    "            table['Files'].append(name[1])\n",
    "\n",
    "        table['Selector'].append(name[-1])\n",
    "        table['N_Samples'].append(n_samples)\n",
    "        table['Test_Size'].append(n_test)\n",
    "        table['Contamination'].append(n_hit)\n",
    "        table['Classifier_1s'].append(link[0])\n",
    "        table['Classifier_2s'].append(link[2])\n",
    "        table['Accuracy'].append(link[7])\n",
    "        table['Recall'].append(link[8])\n",
    "        table['F1'].append(link[9])\n",
    "        table['AUC'].append(link[10])\n",
    "        table['FP'].append(link[5])\n",
    "        table['FN'].append(link[6])\n",
    "        table['FPR'].append(link[5]/(n_test-n_hit))\n",
    "        table['FNR'].append(link[6]/n_hit)\n",
    "        table['Train_Time'].append(link[3])\n",
    "        table['Test_Time'].append(link[4])\n",
    "        table['Iteration'].append(link[1])\n",
    "\n",
    "    done.append(todo)\n",
    "toc = time()\n",
    "timer(toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d30b0c",
   "metadata": {},
   "source": [
    "Salvando tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e601b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftable = pd.DataFrame(table)\n",
    "dftable.to_csv(table_name,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
